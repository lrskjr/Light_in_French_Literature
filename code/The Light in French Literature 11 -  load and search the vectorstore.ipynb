{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3521b18a-08f1-4dc7-a0ba-7faa2034d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -U langchain-openai\n",
    "#! pip install lark\n",
    "#! pip install langchain-community langchain-core\n",
    "#!pip install -U langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ff6ddb-1f59-49c8-ab4f-0595f03ef185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load File Directory\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Document Splitting\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# Embeddings og vectorstores\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# llm søgning\n",
    "from lark import Lark\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "# komprimering\n",
    "import warnings\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "# Search\n",
    "import os\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2477bf-14ec-4534-9684-81b57275e792",
   "metadata": {},
   "source": [
    "# Make Langchain and vectorstore run  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd0d0f5d-4e99-4031-89b3-fc6310d11e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = 'your-api-key-goes-here'\n",
    "\n",
    "client = openai.OpenAI(api_key= OPENAI_API_KEY)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-4o\"): # eller \"gpt-4\" ,   \"gpt-3.5-turbo\", \"gpt-4o\", \"gpt-3.5-turbo-0125\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4e27b-a11f-4d16-a127-c1fd7510e761",
   "metadata": {},
   "source": [
    "# Load the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cb27086-e11c-4744-a147-ae61b3c45f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'docs Light in French Literature/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf9b10e5-f8d5-4ba9-bfbe-555dd539d5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c453bf2f-fc80-4c1d-87d8-be27845afc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41103\n"
     ]
    }
   ],
   "source": [
    "# Antal af dokumenter i samlingen\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5edfe9",
   "metadata": {},
   "source": [
    "# Similarity Search\n",
    "Get the results that are most similar to your question.\n",
    "\n",
    "we use 'k' to deside how many results we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d5f8f3f-fa55-4d2b-bda9-d6e06898ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Find examples where lighted or illuminated spaces tributes to enthusiastic and loving feeling.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7350e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(question,k=4)\n",
    "\n",
    "count = 0\n",
    "for i in docs:\n",
    "    count = count + 1\n",
    "    print ('Example: '+ str(count))\n",
    "    print ('\\n')\n",
    "    print (str(i.page_content).replace('\\n',' ').replace('Ãˆ','È').replace('Ãª', 'ê').replace('â€™','’')\\\n",
    "                              .replace('Å“', 'œ').replace('Ã©', 'é').replace('Ã®','î')\\\n",
    "                                .replace('Ã‰', 'É').replace('Ã¨', 'è').replace('Ã', 'à')\\\n",
    "                                 .replace('à¢', 'â').replace('â€¦', '…').replace('à”', 'Ô')\\\n",
    "                                  .replace('à¹','ù'))\n",
    "    \n",
    "    source = str(i.metadata).replace('}','').replace('.txt','').split('\\\\')[-1]\n",
    "    print (f\"Source: {source}\")\n",
    "    print ('*'*50)\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f2926-6cc4-40ec-abb3-57976acde6fa",
   "metadata": {},
   "source": [
    "# Maximum marginal relevance¶\n",
    "It is not always that you are interested in getting the results that are most similar to your question. Maximum marginal relevance (MMR) is about ensuring diversity in the search results, and achieving both relevance for the query and diversity among the results.\n",
    "\n",
    "The idea is that you send a question to your vector database and get some answers back. We use \"fetch_k=\" to determine how many responses we get back. It is based on semantic similarity alone. From there, we work further with those answers and find the most relevant based on semantic similarity and based on difference. After this we choose \"k=\", which are the answers that will be output.\n",
    "\n",
    "_docs_mmr = vectordb.max_marginal_relevance_search(question,k=3, fetch_k=10)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715efbc4-7009-45d0-bc34-18038fd51df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Find examples where lighted or illuminated spaces tributes to enthusiastic and loving feeling.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3, fetch_k=10)\n",
    "\n",
    "count = 0\n",
    "for i in docs_mmr:\n",
    "    count = count + 1\n",
    "    print ('Example: '+ str(count))\n",
    "    print ('\\n')\n",
    "    print (str(i.page_content).replace('Ãˆ','È').replace('Ãª', 'ê').replace('â€™','’')\\\n",
    "                              .replace('Å“', 'œ').replace('Ã©', 'é').replace('Ã®','î')\\\n",
    "                                .replace('Ã‰', 'É').replace('Ã¨', 'è').replace('Ã', 'à')\\\n",
    "                                 .replace('à¢', 'â').replace('â€¦', '…').replace('à”', 'Ô')\\\n",
    "                                  .replace('à¹','ù'))\n",
    "    \n",
    "    source = str(i.metadata).replace('}','').replace('.txt','').split('\\\\')[-1]\n",
    "    print (f\"Source: {source}\")\n",
    "    print ('*'*50)\n",
    "    print ('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
