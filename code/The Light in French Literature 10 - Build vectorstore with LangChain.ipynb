{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed43a1-c48c-4907-ae81-ef388eed691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('script starts')\n",
    "\n",
    "# Import necessary libraries\n",
    "import psutil  # Used in print_memory_usage function\n",
    "import openai  # Used in get_completion function and main function\n",
    "from tqdm import tqdm  # Used in main function for progress bar\n",
    "import time  # Used in main function for sleep\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings  # Used in main function\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "#from langchain_community.vectorstores import Chroma  # Used in main function\n",
    "from langchain_chroma import Chroma\n",
    "import os  # Used in main function\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to print memory usage\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Memory Usage: {mem_info.rss / (1024 ** 2)} MB\")\n",
    "\n",
    "# Function to get completion from OpenAI\n",
    "def get_completion(prompt, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_completion: {e}\")\n",
    "        return None\n",
    "\n",
    "print('Functions and libraries are loaded')\n",
    "print_memory_usage()\n",
    "\n",
    "# Function to get completion from OpenAI\n",
    "def get_completion(prompt, model=\"gpt-4o\"):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print('Functions and libraries are loaded')\n",
    "print_memory_usage()\n",
    "\n",
    "# Set OpenAI API key\n",
    "OPENAI_API_KEY = 'your-api-key-here'  # Replace with your actual API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "# Set up the embedding and vector database\n",
    "persist_directory = 'docs Light in French Literature/chroma/'\n",
    "embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
    "\n",
    "print('Vector database is loaded')\n",
    "print_memory_usage()\n",
    "\n",
    "# Antal af dokumenter i samlingen\n",
    "print('collection size')\n",
    "print(vectordb._collection.count())\n",
    "\n",
    "# Decide the number of output results\n",
    "K = 10\n",
    "fetch_k = K + 10\n",
    "\n",
    "# Define the question and perform similarity search\n",
    "\n",
    "question = \"Context: these texts sometimes mention artificial lighting, such as lamps, candles, streetlamps, and so on. \\\n",
    "            Please find examples where illumination by artificial lighting contributes to romantic or loving emotions.\"\n",
    "\n",
    "\n",
    "print('Searching starts ... ')\n",
    "# Simulate the fetching process with a progress bar\n",
    "\n",
    "for i in tqdm(range(fetch_k), desc=\"Fetching documents\"):\n",
    "    # Simulate a delay for each fetch\n",
    "    time.sleep(0.1)  # Replace this with the actual fetching logic if possible\n",
    "\n",
    "# Now perform the max marginal relevance search\n",
    "print('perform the max marginal relevance search')\n",
    "try:\n",
    "    docs_mmr = vectordb.max_marginal_relevance_search(question, k=K, fetch_k=fetch_k)\n",
    "    print('Search results stored in docs variable')\n",
    "except Exception as e:\n",
    "    print(f\"Error in max_marginal_relevance_search: {e}\")\n",
    "    docs_mmr = []\n",
    "\n",
    "\n",
    "# Two empyt lists for storing the search output \n",
    "langchain_mmr_results_sources = []\n",
    "langchain_mmr_results_content = []\n",
    "\n",
    "\n",
    "# Process and store the mmr results\n",
    "for count, doc in enumerate(docs_mmr, start=1):\n",
    "    content = doc.page_content\n",
    "    content = (content.replace('\\n', ' ').replace('Ãˆ', 'È').replace('Ãª', 'ê').replace('â€™', '’')\n",
    "                      .replace('Å“', 'œ').replace('Ã©', 'é').replace('Ã®', 'î')\n",
    "                      .replace('Ã‰', 'É').replace('Ã¨', 'è').replace('Ã', 'à')\n",
    "                      .replace('à¢', 'â').replace('â€¦', '…').replace('à”', 'Ô')\n",
    "                      .replace('à¹', 'ù'))\n",
    "\n",
    "    source = doc.metadata.get('source', '').replace('}', '').replace('.txt', '').split('\\\\')[-1]\n",
    "    print (source)\n",
    "    print (content)\n",
    "    print (10 * '*')\n",
    "\n",
    "    # store output\n",
    "    langchain_mmr_results_sources.append(source)\n",
    "    langchain_mmr_results_content.append(content)\n",
    "    \n",
    "print('Clean up variables that are no longer needed')\n",
    "# Clean up variables that are no longer needed\n",
    "del docs_mmr, doc, content, source\n",
    "\n",
    "print('Clean up variables that are no longer needed')\n",
    "print_memory_usage()\n",
    "\n",
    "\n",
    "print('Saving the max marginal relevance search result as csv')\n",
    "df = pd.DataFrame({'source': langchain_mmr_results_sources, 'content':langchain_mmr_results_content})\n",
    "print (df)\n",
    "# save langchain search result\n",
    "df.to_csv('langchain_mmr_search_result.csv', index=False)\n",
    "\n",
    "\n",
    "print('Clean up variables that are no longer needed')\n",
    "# Clean up remaining variables\n",
    "del df, langchain_mmr_results_sources, langchain_mmr_results_content, fetch_k\n",
    "print_memory_usage()\n",
    "\n",
    "print('MMR script done!')\n",
    "\n",
    "\n",
    "print ('sim script starting!')\n",
    "\n",
    "# Two empyt lists for storing the similarity search output \n",
    "langchain_sim_results_sources = []\n",
    "langchain_sim_results_content = []\n",
    "\n",
    "# Process and store the similarity results\n",
    "docs = vectordb.similarity_search(question,k=K)\n",
    "for doc in docs:\n",
    "    content = doc.page_content\n",
    "    content = (content.replace('\\n', ' ').replace('Ãˆ', 'È').replace('Ãª', 'ê').replace('â€™', '’')\n",
    "                      .replace('Å“', 'œ').replace('Ã©', 'é').replace('Ã®', 'î')\n",
    "                      .replace('Ã‰', 'É').replace('Ã¨', 'è').replace('Ã', 'à')\n",
    "                      .replace('à¢', 'â').replace('â€¦', '…').replace('à”', 'Ô')\n",
    "                      .replace('à¹', 'ù'))\n",
    "    \n",
    "    source = doc.metadata.get('source', '').replace('}', '').replace('.txt', '').split('\\\\')[-1]\n",
    "    print (source)\n",
    "    print (content)\n",
    "    print (10 * '*')\n",
    "\n",
    "    # store output\n",
    "    langchain_sim_results_sources.append(source)\n",
    "    langchain_sim_results_content.append(content)\n",
    "\n",
    "print('Clean up variables that are no longer needed')\n",
    "# Clean up variables that are no longer needed\n",
    "del docs, doc, content, source\n",
    "\n",
    "print('Saving the similarity search result as csv')\n",
    "df = pd.DataFrame({'source': langchain_sim_results_sources, 'content':langchain_sim_results_content})\n",
    "print (df)\n",
    "# save langchain search result\n",
    "df.to_csv('langchain_sim_search_result.csv', index=False)\n",
    "\n",
    "print('Sim script done!')\n",
    "\n",
    "\n",
    "print ('Script done.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
